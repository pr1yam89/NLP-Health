{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - Feature Engineering and Supervised Learners\n",
    "\n",
    "In the last few lessons, we learned how python programming and natural language processing (NLP) can be used to process, standardize, and encode textual information that can inform prediction models used to solve a classification task.\n",
    "\n",
    "In this homework, you will demonstrate how to apply these approaches to transform text into informative features to classify sentences by disease status. For our questions, we'll return to our dataset of stroke diagnostic impression sections and associated acute ischemic stroke (AIS vs. non-AIS) classes. \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/paper.png\" width=1000>\n",
    "\n",
    "- Kim C, Zhu V, Obeid J, Lenert L. Natural language processing and machine learning algorithm to identify brain MRI reports with acute ischemic stroke. PLoS One. 2019 Feb 28;14(2):e0212778. doi: 10.1371/journal.pone.0212778. PMID: 30818342; PMCID: PMC6394972.\n",
    "\n",
    "---\n",
    "\n",
    "The total point value is __20 points__. Please submit your completed jupyter notebook through canvas with the following naming convention: _HW2_LastName_FirstName.ipynb_. This assignment will be due no later than __March 27 at 11:59pm__. If you have any questions, please join Wednesday and Thursday's TA hours before reaching directly out to Danielle or Patryk.\n",
    "\n",
    "Good luck!\n",
    "Danielle & Patryk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 1:__ When starting a classification problem, its important to create a development set for training a prediction model and a held-out validation set for assessing the generalizability of the model.  \n",
    "\n",
    "Read in the dataset and randomly sample and create a development (80% of rows) and validation (20% of rows) set. (__**1 point**__)\n",
    "\n",
    "Demonstrate that the development and validation sets are similar by computing and comparing the proportion of AIS and non-AIS cases. (__**2 points**__)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1. No diffusion restriction in brain parenchym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1. Multiple rim- Like nodular enhancement of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1. Diffusion restriction in right parietal occ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>metallic artifact  image distortion loss cere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1. Right temporo-occipital cortical &amp; subcorti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                               Text\n",
       "0   1      0  1. No diffusion restriction in brain parenchym...\n",
       "1   2      0  1. Multiple rim- Like nodular enhancement of b...\n",
       "2   3      1  1. Diffusion restriction in right parietal occ...\n",
       "3   4      0   metallic artifact  image distortion loss cere...\n",
       "4   5      0  1. Right temporo-occipital cortical & subcorti..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### code here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the dataset\n",
    "df = pd.read_csv('pone.0212778.s002.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2419, 3) (605, 3)\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset\n",
    "dev_df, val_df = train_test_split(df, test_size = 0.20, random_state=42, stratify=df['Label'])\n",
    "\n",
    "print(dev_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the proportion of AIS and Non AIS cases\n",
    "\n",
    "def proportion_AIS_non_AIS(dataset):\n",
    "    total_entries = len(dataset)\n",
    "    AIS_entries = len(dataset[dataset['Label'] == 1])\n",
    "    Non_AIS_entries = len(dataset[dataset['Label']== 0])\n",
    "    \n",
    "    AIS_proportion = round((AIS_entries/total_entries)*100,2)\n",
    "    Non_AIS_proportion = round((Non_AIS_entries/total_entries)*100,2)\n",
    "    \n",
    "    return AIS_proportion, Non_AIS_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                AIS Proportion(%)  Non AIS Proportion(%)\n",
      "0  Development              14.30                  85.70\n",
      "1   Validation              14.21                  85.79\n"
     ]
    }
   ],
   "source": [
    "# Calculating proportions for the development set\n",
    "dev_ais_prop, dev_non_ais_prop = proportion_AIS_non_AIS(dev_df)\n",
    "\n",
    "# Calculating proportions for the validation set\n",
    "val_ais_prop, val_non_ais_prop = proportion_AIS_non_AIS(val_df)\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    '': ['Development','Validation'],\n",
    "    'AIS Proportion(%)': [dev_ais_prop,val_ais_prop],\n",
    "    'Non AIS Proportion(%)': [dev_non_ais_prop, val_non_ais_prop]\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 2:__ Before training a model, it can be informative to characterize the dataset to identify textual differences between with each class in the development set. In the paper, they describe differences between the distributions of the overall length of text characters for the impression sections between the non-AIS and AIS radiology reports.\n",
    "\n",
    "\n",
    "<img src=\"img/Text_character_dist.png\" width=500>\n",
    "\n",
    "_Could there be other differences between the texts for each class?_ Write a program that reports the average number of individual sentences within the impression sections by class (AIS vs non-AIS) for the development set. \n",
    "\n",
    "For this question, process the development set to complete the following steps:\n",
    "\n",
    "1. Segment the individual sentences within each impression section (__**3 points**__)\n",
    "2. Determine the number of sentences for each impression section (__**1 point**__)\n",
    "3. Compute and report the average number of sentences within the impressions for both AIS vs non-AIS cases. (__**1 point**__)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Num_Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>2194</td>\n",
       "      <td>0</td>\n",
       "      <td>Unremarkable finding of brain parenchyma and c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>833</td>\n",
       "      <td>0</td>\n",
       "      <td>Target appeared lesion in right corona radiata...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>3015</td>\n",
       "      <td>0</td>\n",
       "      <td>Diffuse brain atrophy\\r\\nNo diffusion restrict...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>2664</td>\n",
       "      <td>0</td>\n",
       "      <td>Unremarkable finding of brain parenchyma and c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>2347</td>\n",
       "      <td>0</td>\n",
       "      <td>Multiple unidentified bright objects or small ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Label                                               Text  \\\n",
       "2193  2194      0  Unremarkable finding of brain parenchyma and c...   \n",
       "832    833      0  Target appeared lesion in right corona radiata...   \n",
       "3014  3015      0  Diffuse brain atrophy\\r\\nNo diffusion restrict...   \n",
       "2663  2664      0  Unremarkable finding of brain parenchyma and c...   \n",
       "2346  2347      0  Multiple unidentified bright objects or small ...   \n",
       "\n",
       "      Num_Sentences  \n",
       "2193              2  \n",
       "832               6  \n",
       "3014              4  \n",
       "2663              2  \n",
       "2346              2  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Function to segment text into sentences and count them\n",
    "def count_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Applying the function to the 'Text' column of the development set and creating a new column 'Num_Sentences'\n",
    "dev_df['Num_Sentences'] = dev_df['Text'].apply(count_sentences)\n",
    "\n",
    "# Displaying the first few rows of the modified development set\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to segment text into sentences using basic delimiters and count them\n",
    "# def basic_sentence_count(text):\n",
    "#     sentences = re.split(r'[.!?]', text)\n",
    "#     # Removing any empty strings resulted from the split\n",
    "#     sentences = [s for s in sentences if s]\n",
    "#     return len(sentences)\n",
    "\n",
    "# # Applying the basic sentence count function to the 'Text' column of the development set\n",
    "# train_X['Num_Sentences_Basic'] = train_X['Text'].apply(basic_sentence_count)\n",
    "\n",
    "# # Displaying the first few rows of the modified development set\n",
    "# train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.606936416184972 5.187650747708635\n"
     ]
    }
   ],
   "source": [
    "# Calculating the average number of sentences for each class (AIS vs non-AIS)\n",
    "avg_sentences_AIS = dev_df[dev_df['Label'] == 1]['Num_Sentences'].mean()\n",
    "avg_sentences_Non_AIS = dev_df[dev_df['Label'] == 0]['Num_Sentences'].mean()\n",
    "\n",
    "print(avg_sentences_AIS, avg_sentences_Non_AIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 3:__ Name 1 advantages and 1 disadvantage to each of the following feature engineering steps:\n",
    "\n",
    "1. stemming words (__**1 point**__)\n",
    "2. generating bigrams (__**1 point**__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stemming words\n",
    "### Advantage: Reduces Dimensionality\n",
    "Stemming helps in reducing the number of unique words in the dataset by converting different forms of a word to a common base form (stem). This reduction in vocabulary size helps in decreasing the dimensionality of the feature space, making the model simpler and less prone to overfitting.\n",
    "\n",
    "### Disadvantage: Loss of Semantics\n",
    "Stemming might lead to words being converted to stems that are not actual words. It can also cause different words with distinct meanings to be stemmed to the same root, leading to a loss of information and potential misinterpretation of the text's semantics.\n",
    "\n",
    "## 2. Generating Bigrams\n",
    "### Advantage: Captures Contextual Information\n",
    "Bigrams help in capturing the contextual information and relationships between adjacent words in the text. Including bigrams as features allows the model to consider word pairs, potentially capturing more meaningful information compared to using only unigrams (single words).\n",
    "\n",
    "### Disadvantage: Increases Dimensionality\n",
    "Generating bigrams significantly increases the number of features as it considers pairs of words. This increase in dimensionality can make the model more complex, require more data to train effectively, and increase the computational cost. It might also make the model more susceptible to overfitting if the dataset is not sufficiently large and diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 4:__ Following the diagram below, recreate the pre-processing and feature engineering steps to create a feature matrix to the development and validation sets: (__**2 points**__)\n",
    "\n",
    "1. Tokenize into single word units \n",
    "2. Reduce the case of each word\n",
    "3. Remove all stopwords\n",
    "4. Stem each word\n",
    "5. Apply TF-IDF weighting\n",
    "6. Generate continuous bi-grams from the stemmed words\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/Pre-processing.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/priyam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#code here \n",
    "\n",
    "#Import Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Downloading the stopwords from NLTK (we will use a basic method if this fails)\n",
    "try:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    stopwords_nltk = set(stopwords.words('english'))\n",
    "except:\n",
    "    stopwords_nltk = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenizing and converting to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Removing stopwords and punctuation, and stemming the words\n",
    "    tokens = [ps.stem(word) for word in tokens if word.isalnum() and word not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>Unremarkable finding of brain parenchyma and c...</td>\n",
       "      <td>unremark find brain parenchyma cerebrospin flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Target appeared lesion in right corona radiata...</td>\n",
       "      <td>target appear lesion right corona radiata diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>Diffuse brain atrophy\\r\\nNo diffusion restrict...</td>\n",
       "      <td>diffus brain atrophi diffus restrict multipl o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>Unremarkable finding of brain parenchyma and c...</td>\n",
       "      <td>unremark find brain parenchyma cerebrospin flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>Multiple unidentified bright objects or small ...</td>\n",
       "      <td>multipl unidentifi bright object small vessel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "2193  Unremarkable finding of brain parenchyma and c...   \n",
       "832   Target appeared lesion in right corona radiata...   \n",
       "3014  Diffuse brain atrophy\\r\\nNo diffusion restrict...   \n",
       "2663  Unremarkable finding of brain parenchyma and c...   \n",
       "2346  Multiple unidentified bright objects or small ...   \n",
       "\n",
       "                                         Processed_Text  \n",
       "2193  unremark find brain parenchyma cerebrospin flu...  \n",
       "832   target appear lesion right corona radiata diff...  \n",
       "3014  diffus brain atrophi diffus restrict multipl o...  \n",
       "2663  unremark find brain parenchyma cerebrospin flu...  \n",
       "2346  multipl unidentifi bright object small vessel ...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the preprocessing function to the development and validation sets\n",
    "dev_df['Processed_Text'] = dev_df['Text'].apply(preprocess_text)\n",
    "val_df['Processed_Text'] = val_df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Displaying the first few rows of the development dataframe with the processed text\n",
    "dev_df[['Text', 'Processed_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the TfidfVectorizer to consider both unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fitting the vectorizer and transforming the development set\n",
    "dev_tfidf = vectorizer.fit_transform(dev_df['Processed_Text'])\n",
    "\n",
    "# Transforming the validation set using the fitted vectorizer\n",
    "val_tfidf = vectorizer.transform(val_df['Processed_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2419, 10242), (605, 10242))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the shapes of the transformed development and validation sets\n",
    "dev_tfidf.shape, val_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 177)\t0.17885086644245832\n",
      "  (0, 3702)\t0.17885086644245832\n",
      "  (0, 367)\t0.17948046893082822\n",
      "  (0, 7697)\t0.10999679381584516\n",
      "  (0, 5459)\t0.10915549693937357\n",
      "  (0, 8673)\t0.25168718383363\n",
      "  (0, 3231)\t0.23312283918811527\n",
      "  (0, 1404)\t0.23098857377479334\n",
      "  (0, 6853)\t0.2339122510508171\n",
      "  (0, 1053)\t0.23234060389619726\n",
      "  (0, 3124)\t0.2339122510508171\n",
      "  (0, 9701)\t0.23331951422118302\n",
      "  (0, 173)\t0.16963884363685464\n",
      "  (0, 3701)\t0.17885086644245832\n",
      "  (0, 345)\t0.10971544656952369\n",
      "  (0, 7696)\t0.10915549693937357\n",
      "  (0, 5458)\t0.10915549693937357\n",
      "  (0, 8655)\t0.22834662489270907\n",
      "  (0, 3214)\t0.21415845175651618\n",
      "  (0, 1403)\t0.23098857377479334\n",
      "  (0, 6850)\t0.23214615118178974\n",
      "  (0, 1043)\t0.19654853151254503\n",
      "  (0, 3122)\t0.3411667356493756\n",
      "  (0, 9695)\t0.18435532564296195\n",
      "  (1, 412)\t0.06733864425102538\n",
      "  :\t:\n",
      "  (2418, 9126)\t0.16051011123761677\n",
      "  (2418, 9746)\t0.24337207671151065\n",
      "  (2418, 4178)\t0.22420992073727802\n",
      "  (2418, 1795)\t0.24337207671151065\n",
      "  (2418, 9744)\t0.22136427916640466\n",
      "  (2418, 9172)\t0.21398237986119087\n",
      "  (2418, 7102)\t0.18933992973594138\n",
      "  (2418, 3627)\t0.1871846310326173\n",
      "  (2418, 688)\t0.13038736696135036\n",
      "  (2418, 5022)\t0.10792667742422768\n",
      "  (2418, 5367)\t0.12211728598871982\n",
      "  (2418, 8775)\t0.1014636209563657\n",
      "  (2418, 7094)\t0.1772194643406878\n",
      "  (2418, 7504)\t0.11260969370618794\n",
      "  (2418, 1361)\t0.08201340756434225\n",
      "  (2418, 5959)\t0.08835437851991433\n",
      "  (2418, 7486)\t0.08174305234061995\n",
      "  (2418, 5957)\t0.0860315631449037\n",
      "  (2418, 3351)\t0.09233152566290559\n",
      "  (2418, 592)\t0.05918023248163418\n",
      "  (2418, 8756)\t0.07557685510233944\n",
      "  (2418, 3248)\t0.06306902587346265\n",
      "  (2418, 449)\t0.10770237673955751\n",
      "  (2418, 1357)\t0.05361437501472856\n",
      "  (2418, 4933)\t0.058194582087515524\n"
     ]
    }
   ],
   "source": [
    "print(dev_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 5:__ Name 2 supervised learners used in the manuscript. (__**1 point**__)\n",
    "\n",
    "Define recall and precision. Explain the advantage of these measures over use of accuracy and when its most important to report them (**2 points**) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "__Question 6:__ Train a Naive Bayes classifier using the development set.  Apply the trained model to the validation set. Report precision, recall, and F1-score. \n",
    "\n",
    "_Note that you can use features from Question 4's approach, a simple CountVectorizer, or create your own features)._\n",
    "(**5 points**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:  0.046511627906976744\n",
      "F1 Score:  0.08888888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93       519\n",
      "           1       1.00      0.05      0.09        86\n",
      "\n",
      "    accuracy                           0.86       605\n",
      "   macro avg       0.93      0.52      0.51       605\n",
      "weighted avg       0.88      0.86      0.81       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#code here \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Initializing the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Training the classifier using the development set\n",
    "nb_classifier.fit(dev_tfidf, dev_df['Label'])\n",
    "\n",
    "# Making predictions on the validation set\n",
    "val_predictions = nb_classifier.predict(val_tfidf)\n",
    "\n",
    "# Calculating precision, recall, and F1-score for the validation set\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(val_df['Label'], val_predictions, average='binary')\n",
    "\n",
    "# Getting a detailed classification report\n",
    "classification_rep = classification_report(val_df['Label'], val_predictions)\n",
    "\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1 Score: \",f1_score) \n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
